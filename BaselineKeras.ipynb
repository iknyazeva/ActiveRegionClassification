{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import keras\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from NNutils import create_fname_dict, train_val_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data ='ALLrescaled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPath = 'path_to_foulder'\n",
    "path_to_data = 'path_to_data'\n",
    "df = pd.read_pickle(os.path.join(dfPath,'infoDf.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>instr_type</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>number_of_ss</th>\n",
       "      <th>size</th>\n",
       "      <th>class</th>\n",
       "      <th>location</th>\n",
       "      <th>magn_class</th>\n",
       "      <th>letter_1</th>\n",
       "      <th>letter_2</th>\n",
       "      <th>letter_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1996-06-10</th>\n",
       "      <th>7971</th>\n",
       "      <td>MDI</td>\n",
       "      <td>650.25</td>\n",
       "      <td>-263.82</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>BXO</td>\n",
       "      <td>S07W15</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>X</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1996-06-27</th>\n",
       "      <th>7973</th>\n",
       "      <td>MDI</td>\n",
       "      <td>635.91</td>\n",
       "      <td>-2353.96</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>HSX</td>\n",
       "      <td>N09W21</td>\n",
       "      <td>A</td>\n",
       "      <td>H</td>\n",
       "      <td>S</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7976</th>\n",
       "      <td>MDI</td>\n",
       "      <td>423.33</td>\n",
       "      <td>-1444.87</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>AXX</td>\n",
       "      <td>N13E25</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1996-06-28</th>\n",
       "      <th>7976</th>\n",
       "      <td>MDI</td>\n",
       "      <td>569.68</td>\n",
       "      <td>-1676.75</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>HRX</td>\n",
       "      <td>N13E11</td>\n",
       "      <td>A</td>\n",
       "      <td>H</td>\n",
       "      <td>R</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7973</th>\n",
       "      <td>MDI</td>\n",
       "      <td>448.23</td>\n",
       "      <td>-2075.18</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>HSX</td>\n",
       "      <td>N08W35</td>\n",
       "      <td>A</td>\n",
       "      <td>H</td>\n",
       "      <td>S</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  instr_type     max      min number_of_ss size class  \\\n",
       "date       region                                                       \n",
       "1996-06-10 7971          MDI  650.25  -263.82            2   10   BXO   \n",
       "1996-06-27 7973          MDI  635.91 -2353.96            1   70   HSX   \n",
       "           7976          MDI  423.33 -1444.87            4   10   AXX   \n",
       "1996-06-28 7976          MDI  569.68 -1676.75            4   20   HRX   \n",
       "           7973          MDI  448.23 -2075.18            1   70   HSX   \n",
       "\n",
       "                  location magn_class letter_1 letter_2 letter_3  \n",
       "date       region                                                 \n",
       "1996-06-10 7971     S07W15          B        B        X        O  \n",
       "1996-06-27 7973     N09W21          A        H        S        X  \n",
       "           7976     N13E25          A        A        X        X  \n",
       "1996-06-28 7976     N13E11          A        H        R        X  \n",
       "           7973     N08W35          A        H        S        X  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_dict, label_encoder = create_fname_dict(df,'letter_1')\n",
    "val_name_dict, train_name_dict  = train_val_split(names_dict, share = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_name_dict, train_name_dict = train_val_split(names_dict, share = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_by_name_dict(name_dict, shuffle = True):\n",
    "    \n",
    "    fnames = []\n",
    "    Xlist = []\n",
    "    ylist = []\n",
    "    for key in name_dict.keys():\n",
    "        print('Process: '+ key)\n",
    "        for fname in tqdm(name_dict[key]):\n",
    "            hdulist = fits.open(fname)\n",
    "            X =  hdulist[0].data\n",
    "            y = label_encoder.transform([key])\n",
    "            y = np.squeeze(keras.utils.to_categorical(y, num_classes=len(val_name_dict.keys())))\n",
    "            X = X.reshape(1, X.shape[0], X.shape[1]).astype('float32')\n",
    "            Xlist.append(X)\n",
    "            ylist.append(y)\n",
    "            fnames.append(fname)\n",
    "    \n",
    "    print('Merge all data to array')\n",
    "    y = np.array(ylist)\n",
    "    fnames = np.array(fnames)    \n",
    "    x_stats = np.array([log_stats(x) for x in Xlist])\n",
    "        #x_log = np.array([np.sign(x)*np.log1p(np.abs(x)) for x in batch_X])/8.24\n",
    "    X= np.array(Xlist)/3900\n",
    "    if shuffle:\n",
    "        len_ = y.shape[0]\n",
    "        idx = np.arange(len_)\n",
    "        np.random.shuffle(idx)\n",
    "        return ([x_stats[idx], X[idx]], y[idx], fnames[idx])\n",
    "    else:\n",
    "        return ([x_stats, X], y,fnames) \n",
    "def log_stats(x):\n",
    "    stats = np.percentile(x,[0.1, 1,5,10,90,95,99,99.9])\n",
    "    return np.sign(stats)*np.log1p(np.abs(stats))/8.24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Input, Concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization, regularizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.pooling import AveragePooling2D\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model(fsize=5, nb_filters = 64):\n",
    "    '''Base network to be shared (eq. to feature extraction).\n",
    "    '''\n",
    "    input_seq = Input(shape=(1,100,125))\n",
    "    droprate = 0.3\n",
    "    #nb_filters  = 150\n",
    "    convolved = Conv2D(nb_filters, (fsize, fsize), activation=\"tanh\")(input_seq)\n",
    "    convolved = AveragePooling2D()(convolved)\n",
    "    convolved  = BatchNormalization()(convolved)\n",
    "    convolved = Conv2D(nb_filters, (fsize, fsize), activation=\"tanh\")(convolved)\n",
    "    convolved = AveragePooling2D()(convolved)\n",
    "    convolved  = BatchNormalization()(convolved)\n",
    "    convolved = Conv2D(nb_filters, (fsize, fsize), activation=\"tanh\")(convolved)\n",
    "    convolved = AveragePooling2D()(convolved)\n",
    "    convolved  = BatchNormalization()(convolved)\n",
    "    processed = Flatten()(convolved)\n",
    "    compressed = Dense(50, activation=\"tanh\")(processed)\n",
    "    #compressed = Dense(50, activation=\"tanh\")(processed)\n",
    "    compressed = Dropout(droprate)(compressed)\n",
    "    #compressed = Dense(20, activation=\"tanh\")(compressed)\n",
    "    model = Model(inputs=input_seq, outputs=compressed)            \n",
    "    return model\n",
    "def main_model(fsize=5, nclass = 7, nb_filters = 64):\n",
    "    input_1 = Input(shape = (8,))\n",
    "    input_2 = Input(shape=(1,100,125))\n",
    "\n",
    "    c_model = conv_model(fsize, nb_filters)\n",
    "    embed1 = Dense(50, activation=\"tanh\")(input_1)\n",
    "    embed2 = c_model(input_2)\n",
    "\n",
    "    merged = Concatenate()([embed1, embed2])\n",
    "    out = Dense(nclass, activation='softmax')(merged)\n",
    "    model = Model(inputs=[input_1, input_2], outputs=out)\n",
    "    return model\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))    \n",
    "def learn_letter1(BS=500, nb_epoch=100):\n",
    "    loss_history = LossHistory()\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', patience=20, verbose=0, mode='auto')\n",
    "    filepath=\"letter1-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [loss_history, earlyStopping,checkpoint]\n",
    "    my_model= main_model()\n",
    "    my_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    my_model.fit(Xtrain, ytrain_let1,batch_size = BS, epochs=nb_epoch, \n",
    "                      callbacks=callbacks_list, verbose=1, shuffle = True, validation_data=(Xval,yval_let1))\n",
    "    return my_model, loss_history \n",
    "def learn_letter2(BS=500, nb_epoch=100):\n",
    "    loss_history = LossHistory()\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', patience=20, verbose=0, mode='auto')\n",
    "    filepath=\"letter2-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [loss_history, earlyStopping,checkpoint]\n",
    "    my_model= main_model(nclass = yval_let2.shape[1])\n",
    "    my_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "    BS = 500\n",
    "    nb_epoch = 100\n",
    "\n",
    "    my_model.fit(Xtrain, ytrain_let2,batch_size = BS, epochs=nb_epoch, \n",
    "                      callbacks=callbacks_list, verbose=1, shuffle = True, validation_data=(Xval,yval_let2))\n",
    "    return my_model, loss_history\n",
    "def learn_letter3(BS=500, nb_epoch=100):\n",
    "    loss_history = LossHistory()\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', patience=20, verbose=0, mode='auto')\n",
    "    filepath=\"letter3-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [loss_history, earlyStopping,checkpoint]\n",
    "    my_model= main_model(nclass = yval_let3.shape[1])\n",
    "    my_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "    BS = 500\n",
    "    nb_epoch = 100\n",
    "\n",
    "    my_model.fit(Xtrain, ytrain_let3,batch_size = BS, epochs=nb_epoch, \n",
    "                      callbacks=callbacks_list, verbose=1, shuffle = True, validation_data=(Xval,yval_let3))\n",
    "    return my_model, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "let_1_model, loss_history1 = learn_letter1(BS=500, nb_epoch=200)\n",
    "let_2_model, loss_history2 = learn_letter2(BS=500, nb_epoch=200)\n",
    "let_3_model, loss_history3 = learn_letter3(BS=500, nb_epoch=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
